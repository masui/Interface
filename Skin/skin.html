<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>スキン利用インタフェース</title>
</head>
<body style="width:500px;">

スマートホンやタブレットPCなど、
タッチパネルを使う製品が増えており、
タッチ操作にもとづくアプリケーションやサービスも増えてきています。
ペンやマウスを使わなくても計算機を操作できる点でこれらの機器は便利ですが、
タッチパネルのついた機器をを持ち歩く必要があります。
何も持ち歩かなくても様々なタッチ操作を行なうことができれば便利でしょう。

<!-- 皮膚の電気的特性を利用したシステムは従来から様々なものが用いられていますが、-->
人体を通信媒体として利用する「人体通信」や、
自分の皮膚をタッチパネルのように利用するシステムが最近注目を集めています。

<!-- 人体を通信媒体として使ったり入力装置として使ったりする研究が流行してきています。 -->

<h2>人体通信</h2>

人体を通信媒体として利用する
「人体通信」(PAN: Personal Area Network)が
1995年ごろから注目されはじめました[1]。

人体通信とは、人間の手や腕で何かに触れたとき、手や腕の電流または電界を利用して
通信を行なうというものです。

体のどこかに自分の情報を記録した装置を装着しておけば、
手を経由してその情報を伝えることによって
握手しただけで名刺交換を行なうことができます。

体のどこかにSuicaと同様の情報をもつ装置を持っておき、
改札口にタッチしたとき情報がゲートに伝わるようにしておけば
Suicaのようなカードを利用することなく
認証を行なって改札口を通ることができるようになるでしょう。

タッチしたものから指や手を経由して情報を取り出せるようになっていれば、
何かに触れることによって情報を取得して拡張現実感(AR)のような使い方をしたり、
さわったものを記録してライフログのような使い方ができるかもしれません。

<!--
http://www.kddi.com/business/oyakudachi/square/labo/004/index.html
KDDIの人体通信
-->
<h2>体表を入力装置として利用する</h2>

体表を通信媒体として利用するだけではなく、
体表をタッチパネルのような入力装置として利用することができれば
タブレット装置を持ち歩く必要がなくなるはずです。
体表を入力装置にするというのは奇抜な考え方ですが、
そのような手法の研究が近年盛んになってきています。

<h3>Skinput</h3>

カーネギーメロン大学のChris Harrison氏は、
腕や手を別の手の指でタップしたときの音を認識することにより
どこの部位がタップされたかを高精度で認識し、
メニュー操作などのコンピュータインタラクションを実現する
Skinputというシステムを提案しています[2]。

指で皮膚をタップすると、
タップの強さや場所に応じて
波紋のように伝達する横波が発生します。
また、タップのエネルギーの一部は縦波として骨に向かって伝達されて骨の周囲の組織を揺らし、
新たな縦波が生成されます。

直接腕の表面を伝わる横波と、一度骨の中に入ってから骨の外に出る縦波
という2種類の波を解析することにより、
タップされた位置をある程度正確に知ることができます。

<p>
<img src="http://gyazo.com/5b37922ca6dd89191fcc3d94f20c0ae4.png">
<p>

上図はSkinputを使ってメニュー操作を行なっているところです。
上腕部に取り付けられたセンサでタップ位置を取得し、
小型プロジェクタで画像を投影することにより
腕タップでメニューを利用できるようになっています。

<!--
http://www.chrisharrison.net/index.php/Research/Skinput
Skinput: Appropriating the Body as an Input Surface
LunarModule氏の解説
http://blog.livedoor.jp/lunarmodule7/archives/981829.html
-->

<!--
<h3>TapSense</h3>

TapSense (C. Harrison)
This is achieved by segmenting and classifying sounds resulting from a finger’s impact. 
http://www.chrisharrison.net/index.php/Research/TapSense/
-->

<h3>手の甲インタフェース</h3>

東大の中妻啓氏と慶應義塾大学の牧野泰才氏は、
左手の甲を右手の指でタップしたときの振動と音を検出することによって
左手の甲をタッチパッドのように利用するシステムを提案しています[3]。

腕時計のような装置にセンサが組み込んであり、
赤外線の反射で指の位置を検出したり、
手をタップする振動をピエゾ素子で検出したり、
手の甲を指でこする音を検出して操作を区別したり
することができます。

<p>
<img src="http://www.alab.t.u-tokyo.ac.jp/~shinolab/wordpress/wordpress/wp-content/uploads/2011/06/SIGGRAPH2011ETechWristBand.jpg" width=400">
<p>

<h3>Touché</h3>

<!--
http://www.satomunehiko.com/?page_id=850
Touché: Enhancing Touch Interaction on Humans, Liquids, and Everyday Objects
http://masui.sfc.keio.ac.jp/2c644ed4d479d2fc320a0619800144ae.pdf

> こちらのページにある図は、出典を記載頂ければご利用頂いて問題ありません。
> http://www.disneyresearch.com/research/projects/hci_touche_drp.htm
> どうぞよろしくお願い致します。
-->

東大の佐藤宗彦氏は、様々なタッチ操作を検出することができる
「Touché」というセンサ技術を提案しています[4]。

普通の静電容量型タッチセンサでは、
指が触れているかいないかの2種類の状態を区別することしかできませんが、
Touchéでは
Swept Frequency Capacitive Sensing
(SFCS: 広範囲の周波数帯での静電容量計測)という技術を利用することにより、
様々なジェスチャを認識することができます。
1KHzから3.5MHzまでの周波数の信号を17.5KHzステップでスイープ出力し、
各周波数における減衰のエンベロープを測定することによって
どのような状態でタッチが行なわれているかを検出します。
たとえば、ドアノブを1本の指で触れている/2本の指でつまんでいる/握っている/
触れずに手をかざしている、といった状態を区別することが可能なので、
ドアノブをどのように握るかによって異なるインタラクションを提供することが
可能になっています。

<!--
> TouchéはSwept Frequency Capacitive Sensing (SFCS: 広範囲の周波数帯での
> 静電容量計測)を行うことで、ほぼすべての物体でジェスチャ入力を可能とする
> センシング技術です。従来の静電容量タッチセンサでは、指が触れているかい
> ないかの2種類の状態しか分かりませんが、SFCSを用いることで、ドアノブを一
> 本の指で触れている・二本の指でつまんでいる、握っている、触れずに手をか
> ざしている、といった複雑なタッチジェスチャを非常に高精度で検出すること
> が可能となります。導電性のある物体であればセンサから一本のワイヤーを取
> り付けるだけでどんなものでもジェスチャ検出が可能となります。そのため、
> タッチスクリーン上でのインタラクションに限らず、ドアノブや水などの液体、
> 人体そのものもタッチインタラクティブにすることが出来ます。本論文では、
> 5種類の条件でのインタラクションを提案しており、99%以上の精度でジェス
> チャーを判別可能であることも示しています。
-->

<p>
<img src="http://gyazo.com/78dec99beeba7f509f4e9e6bfdcfd90a.png" width=400>
<br>
<img src="http://www.disneyresearch.com/research/projects/Touche/touche-diagram-large.jpg" width=400>
<p>

<h2>体に装置を埋め込む</h2>

体表を入力装置として使う場合でも
なんらかのセンサを体に装着する必要がありますが、
センサや回路すべてを体の中に埋め込んでしまえば
本当に何も体に装着する必要がなくなるはずです。

常にONになっているセンサや計算機を体に「着て」使うという
「ウェアラブルコンピューティング」が一時話題になりましたが、
計算機を身につけるのではなく体に埋め込んでしまおうという
「インプラントコンピューティング」を考えている人もいます。
体の中に機械を埋め込むのは嫌だと思う人が現在は多数派だと思われますが、
ペースメーカを埋め込んで生活している人もいるわけですし、
本当に便利になるのであれば
RFIDなどを体内に埋め込んで使うことを選択する人もいるかもしれません。

<p>
<!--
http://www.christianholz.net/implanted_user_interfaces.html
-->
AutoDeskのChristian Holz氏らは、
腕に埋め込んで利用するためのデバイスを試作し、
それによってどのようなインタラクションが可能になるかの実験を行なっています[5]。
まだ自分の腕に埋め込むには到っておらず、
人造皮膚を使った実験しか行なっていないようですが、
様々な可能性が示されています。

<p>
<img src="http://gyazo.com/85cd094e32b67214a4b75c9a00b38327.png">
<p>

装置を利用することなく、
誰がどこで何をどのように触ったかという情報を得ることができるようになれば、
今までに無かった形の様々なユビキタスコンピューティングが可能になるかもしれません。
人体を利用した通信や入力の今後の研究に期待したいと思います。

<p>
<hr>
<p>
<h3>参考文献</h3>
<br>
[1] Zimmerman, T.G., Smith, J.R., Paradiso, J.A., Allport, D.,
Gershenfeld, N., Applying electric field sensing to human-computer
interfaces. in Proceedings of CHI '95, pp. 280-287.
<br>
[2] Harrison, C., Tan, D. Morris, D. Skinput: Appropriating the
Body as an Input Surface. In Proceedings of CHI'10, pp. 453-462.
<br>
[3] http://www.alab.t.u-tokyo.ac.jp/~shinolab/projects/touchinterfaceonbackofhand/
<br>
[4] Sato, M., Poupyrev, I, and Harrison, C. Touché: Enhancing Touch
Interaction on Humans, Screens, Liquids, and Everyday Objects. In
Proceedings of CHI'12. 2012.
<br>
[5] Christian Holz, Tovi Grossman, George Fitzmaurice, Anne
Agur. Implanted User Interfaces. In Proceedings of CHI'12.
pp. 503–512.

</body>
</html>
